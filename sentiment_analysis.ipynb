{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "This Jupyter notebook is an almost-complete transcription of the article \"Sentiment Analysis: First Steps With Python's NLTK Library\" written by Marius Mogyorosi for RealPython. \n",
    "\n",
    "The article can be found at https://realpython.com/python-nltk-sentiment-analysis/ \n",
    "\n",
    "\n",
    "I have removed and/or altered some portions of the original article to better fit the context of a Jupyter notebook.\n",
    "-->\n",
    "# __Sentiment Analysis in Python using NLTK__\n",
    "\n",
    "### <u>Introduction to NLTK</u>\n",
    "\n",
    "NLTK (Natural Language ToolKit) is a Python library which contains a variety of utilities that allow you to effectively manipulate and analyze linguistic data. Among its advanced features are __text classifiers__ that you can use for many kinds of classification, including sentiment analysis.\n",
    "\n",
    "__Sentiment analysis__ is the practice fo using algorithms to classify various samples of related text into overall positive and negative categories. With NLTK, you can employ these algorithms through powerful built-in machine learning operations to obtain insights from linguistic data.\n",
    "\n",
    "After using <code>pip</code> to install the <code>nltk</code> library, execute the following code to download the corpora used in this notebook.\n",
    "\n",
    "> <u>__Note</u>:__\n",
    "> The word __corpus__ and its plural form, __corpora__ refer to large collections of related text samples. In the context of NLTK, corpora are compiled with features for [natural language processing (NLP)](https://en.wikipedia.org/wiki/Natural_language_processing), such as categories and numerical scores for particular features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download([\n",
    "   \"names\",\n",
    "   \"stopwords\",\n",
    "   \"state_union\",\n",
    "   \"twitter_samples\",\n",
    "   \"movie_reviews\",\n",
    "   \"averaged_perceptron_tagger\",\n",
    "   \"vader_lexicon\",\n",
    "   \"punkt\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpora being used in this notebook are:\n",
    "\n",
    "- __names__: A [list of common English names](https://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/) compiled by Mark Kantrowitz\n",
    "- __stopwords__: A list of really common words, like articles, pronouns, prepositions and conjunctions\n",
    "- __state_union__: A sample of transcribed [State of the Union](https://en.wikipedia.org/wiki/State_of_the_Union) addresses by different US presidentsm compiled by Kathleen Ahrens\n",
    "- __twitter_samples__: A list of social media phrases posted to Twitter\n",
    "- __movie_reviews__: [Two thousand movie reviews](https://www.cs.cornell.edu/people/pabo/movie-review-data/) categorized by Bo Pang and Lillian Lee\n",
    "- __averaged_perceptron_tagger__: A data model that NLTK uses to categorize words into their [part of speech](https://en.wikipedia.org/wiki/Part_of_speech)\n",
    "- __vader_lexicon__: A scored [list of words and jargon](https://github.com/cjhutto/vaderSentiment) that NLTK references when performing sentiment analysis, created by C.J. Hutto and Eric Gilbert\n",
    "- __punkt__: A data model created by Jan Strunk that NLTK uses to split full texts into word lists.\n",
    "\n",
    "To view all of the available corpora and pre-trained models, run <code>nltk.download()</code>.\n",
    "\n",
    "If NLTK requires resources that you have yet to install, you'll see a <code>LookupError</code> with details and instructions to download the resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.corpus.shakespeare.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### <u>Compiling Data</u>\n",
    "\n",
    "NLTK provides a number of functions that you can call with few or no arguments that will help you meaningfully analyze text before you even touch its machine learning capabilities. Many of NLTK's utilities are helpful in preparing your data for more advanced analysis.\n",
    "\n",
    "Soon, you'll learn about frequency distributions, concordance, and collocations. But first, you need some data.\n",
    "\n",
    "Start by loading the State of the Union corpus that was downloaded earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in nltk.corpus.state_union.words() if word.isalpha()]\n",
    "\n",
    "# This is equivalent to:\n",
    "# words = []\n",
    "# for word in nltk.corpus.state_union.words():\n",
    "#     if word.isalpha():\n",
    "#         words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you build a list of individual words with the corpus's <code>.words()</code> method, but you use <code>str.isalpha()</code> to include only the words that are made up of letters. Otherwise, your words list may end up with \"words\" that are only punctuation marks.\n",
    "\n",
    "Have a look at your list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint   # pretty print\n",
    "\n",
    "pprint(words[:25], compact=True)   # This is an extraordinarly long list, \n",
    "                     # so we'll stick to 50 words here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice lots of little words like \"of\", \"a\", \"the\", and similar. These common words are called __stop words__, and they can have a negative effect on your analysis because they occur so often in the text. Thankfully, there's a convenient way to filter them out.\n",
    "\n",
    "NLTK provides a small corpus of stop words that you can load into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to specify <code>english</code> as the desired language since this corpus contains stop words in various languages.\n",
    "\n",
    "Now, you can remove stop words from your original word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in words if word.lower() not in stopwords]\n",
    "\n",
    "pprint(words[:25], compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also remove stop words from the list on creation\n",
    "words2 = [word for word in nltk.corpus.state_union.words() \n",
    "         if word.isalpha() and word.lower() not in stopwords]\n",
    "print('\\n')\n",
    "pprint(words2[:25], compact=True)\n",
    "\n",
    "print('\\n', words == words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all words in the <code>stopwords</code> list are lowercase, and those in the original list may not be, you use <code>str.lower()</code> to account for any discrepancies. Otherwise, you may end up with mixedCase or capitalized stop words still in your list.\n",
    "\n",
    "While the corpora used in this notebook are provided by NLTK, it's possible to build your own text corpora from any source. Building a corpus can be as simple as loading some plain text or as complex as labeling and categorizing each sentence. Refer to NLTK's documentation for more information on [how to work with corpus readers](https://www.nltk.org/howto/corpus.html).\n",
    "\n",
    "For some quick analysis, creating a corpus could be overkill. If all you need is a word list, there are simpler ways to achieve this goal. Beyond Python's own string manipulation methods, NLTK provides <code>nltk.word_tokenize()</code>, a function that splits raw text into individual words. While __tokenization__ is itself a bigger topic (and likely one of the steps you'll take when creating a custom corpus), this tokenizer delivers simple word lists really well.\n",
    "\n",
    "To use it, call <code>word_tokenize()</code> with the raw text you want to split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"For some quick analysis, creating a corpus could be overkill. If all you need is a word list, there are simpler ways to achieve this goal.\"\n",
    "\n",
    "pprint(nltk.word_tokenize(text), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a workable word list! Remember that punctuation will be counted as individual words, so use <code>str.isalpha()</code> to filter them out later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### <u>Creating Frequency Distributions</u>\n",
    "\n",
    "Now you're ready for __frequency distributions__. A frequency distribution is essentially a table that tells you how many times each word appears within a given text. In NLTK, frequency distributions are a specific object type implemented as a distinct class called <code>FreqDist</code>. This class provides useful operations for word frequency analysis.\n",
    "\n",
    "To build a frequency distribution with NLTK, construct the <code>nltk.FreqDist</code> class with a word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the words list from above\n",
    "\n",
    "fd = nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a frequency distribution object similar to a [Python dictionary](https://realpython.com/python-dicts/) but with added features.\n",
    "\n",
    "After building the object, you can use methods list <code>.most_common()</code> and <code>.tabulate()</code> to start visualizing information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(fd.most_common(8), compact=True)\n",
    "print('\\n')\n",
    "fd.tabulate(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods allow you to quickly determine frequently used words in a sample. With <code>.most_common()</code>, you get a list of tuples containing each word and how many times it appears in your text. You can get the same information in a more readable form with <code>.tabulate()</code>.\n",
    "\n",
    "In addition to these two methods, you can use frequency distributions to query particular words. You can also use them as iterators to perform some custom analysis on word properties.\n",
    "\n",
    "For example, to discover differences in case, you can query for different variations of the same word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fd['America'])\n",
    "\n",
    "print(fd['america'])   # Note this doesn't result in a KeyError\n",
    "\n",
    "print(fd['AMERICA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These return values indicate the number of times each word occurs exactly as given.\n",
    "\n",
    "Since frequency distribution objects are [iterable](https://realpython.com/python-for-loop/#iterables), you can use them within [list comprehenstions](https://realpython.com/list-comprehension-python/) to create subsets of the initial distribution. You can focus these subsets on properties that are useful for your own analysis.\n",
    "\n",
    "Try creating a new frequency distribution that's based on the initial one but normalizes all words to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_fd = nltk.FreqDist([word.lower() for word in words])\n",
    "\n",
    "lower_fd.tabulate(8)   # Note that some of the most common words have increased in frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a more accurate representation of word usage regardless of case.\n",
    "\n",
    "NLTK provides many other ways to represent data from the frequency distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Unique words:\\n {lower_fd.B()}')  # Number of unique words\n",
    "\n",
    "print(f'Total words:\\n {lower_fd.N()}')  # Total number of words\n",
    "\n",
    "print(f'Fraction of words that are \\'america\\':\\n {lower_fd.freq('america')}')  # Frequency (as a decimal) of a given word\n",
    "\n",
    "lower_fd.plot(10)   # You may need to install the 'matplotlib' package first for this to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### <u>Extracting Concordance and Collocations</u>\n",
    "\n",
    "In the context of NLP, a __concordance__ is a collection of word locations along with their context. You can use concordances to find:\n",
    "1. How many time a word appears\n",
    "2. Where each occurrence appears\n",
    "3. What words surround each occurrence\n",
    "\n",
    "In NLTK, you can do this by calling <code>.concordance()</code>. To use it, you need an instance of the <code>nltk.Text</code> class, which can also be constructed with a word list.\n",
    "\n",
    "Before invoking <code>.concordance()</code>, build a new word list from the original corpus text so that all the context, even stop words, will be there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(nltk.corpus.state_union.words())\n",
    "\n",
    "text.concordance('america', lines=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
